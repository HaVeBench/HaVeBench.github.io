<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>HaVeBench</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <style>
    body {
      background-color: #fafafa;
    }
    hr {
      background-color: #f1f1f1;
    }
    .image-container {
      width: 100%;
      margin-top: 1em;
      text-align: center;
    }
    .image-container img {
      max-width: 100%;
      height: auto;
    }
    .table th {
      padding: 0.2em 0.2em 0 0.2em;
      width: 50%;
    }
    .table td {
      padding: 0.25em;
      width: 50%;
    }
    .content-block {
      padding: 0 6px;
    }
    .title {
      font-size: 32px;
      padding-top: 32px;
    }
    .publisher {
      margin-bottom: 1rem;
    }
    .sub-title {
      font-size: 26px;
      padding-bottom: 16px;
    }
    .subsub-title {
      font-size: 20px;
      padding-bottom: 8px;
    }
    .author {
      font-size: 16px;
    }
    .task {
      padding-bottom: 12px;
    }
    /* .has-text-centered img {
      max-width: 100%;
      height: auto;
    } */
  </style>
  <script type="importmap">
    {
      "imports": {
        "vue": "https://unpkg.com/vue@3/dist/vue.esm-browser.js",
        "three": "https://unpkg.com/three@0.127.0/build/three.module.js",
        "three/addons/": "https://unpkg.com/three@0.127.0/examples/jsm/"
      }
    }
  </script>
</head>

<body>
  <div id="app">
    <div class="columns">
      <div id="main-content" class="column is-6 is-offset-3" :class="[content, offset]">
        <h1 class="title has-text-centered" style="margin-bottom: 0.5em">
          HaVeBench: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models
        </h1>

        <!-- <p class="publisher has-text-centered">Conference</p> -->

        <div class="has-text-centered" style="padding: 0 2em;">

          <p class="author">
            <a href="https://patrick-tssn.github.io/">Yuxuan Wang</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
            <a href="https://github.com/yellow-binary-tree">Yueqian Wang</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
            <a href="https://cihangxie.github.io/">Cihang Xie</a><sup>3</sup> &nbsp;&nbsp;&nbsp;
            <a href="https://zilongzheng.github.io/">Zilong Zheng</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
          </p>

          <!-- <p style="font-size: 0.9em; padding: 0.5em 0;">âœ¶ indicates equal contribution</p> -->

          <p style="font-size: 1em;">
            <sup>1</sup>Beijing Institute for General Artificial Intelligence (BIGAI)<br>
            <sup>2</sup>Peking University&nbsp;&nbsp;&nbsp;<sup>3</sup>UC Santa Cruz<br>
          </p>

          <div style="margin: 1em 0; font-size: 1.1em;">
            <p> <a href="https://arxiv.org/abs/xxxx">arXiv</a> &nbsp;| <a href="https://github.com/patrick-tssn/HaVeBench">Code</a> &nbsp;</p>
          </div>
        </div>

        <div class="has-text-centered" style="padding: 0 1em;">
          <img src="assets/teaser.JPG"/>
        </div>

        <hr>

        <div class="has-text-centered content-block">
          <h2 class="sub-title">Abstract</h2>
          <p style="text-align:justify; margin-bottom: 1.5em;">
            Recent advancements in Multimodal Large Language Models have extended their capabilities to video understanding, yet they are often plagued by ``hallucinations,'' where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces HaVeBench, the first comprehensive benchmark for hallucination detection in video-language models. HaVeBench categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic fact, and extrinsic non-fact hallucinations. Moreover, we adopt an adversarial binary Video-Question-Answering method, where we strategically craft pairs of basic and hallucinated questions, for robust evaluation. By comprehensively evaluating 9 video-language models on HaVeBench, we interestingly reveal that 1) the majority of current models exhibit significant issues with hallucinations; 2) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting temporal and extrinsic factual hallucinations; 3)  existing models are more adept at detecting facts than they are at identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average 7.89\% improvement in hallucination resistance across multiple model architectures. The complete evaluation suite is included in the supplementary material.
          </p>
        </div>

        <hr>

        <div id="results" class="content-block">
          <h2 class="sub-title has-text-centered">HaVeBench data sample</h2>
          
          <div class="image-container">
            <select id="imageSelector" class="select is-medium" v-model="selectedImage">
              <option value="assets/teaser.JPG">Teaser</option>
              <option value="assets/sunburst.png">Sunburst</option>
              <!-- Add more options for images -->
            </select>
            <!-- Use v-bind for src and alt attributes to make them reactive -->
            <img v-bind:src="selectedImage" v-bind:alt="selectedImageAlt"/>
          </div>
        </div>

        <hr>

        <div class="has-text-centered content-block">
          <h2 class="sub-title">Bibtex</h2>
          <p style="text-align:justify">If you find our project useful, please consider citing us:</p>
          <div style="padding-top: 1rem; text-align: left;">
<pre><code>
@inproceedings{xxx,
    title={xxx},
    author={xxx},
    booktitle={xxx},
    year={xxx},
    url={xxx}
}
</code></pre>
          </div>
        </div>

      </div>
    </div>

    <footer class="footer" style="padding: 0 0 1.5rem 0">
      <div class="columns">
        <div class="column" :class="[content, offset]">
          <hr>
          <p class="has-text-centered" style="font-size: 0.9em;">This website is designed and coded by the authors (thanks <a href="https://sqa3d.github.io/">SQA3D</a> for the template). Send feedback and questions to <a href="https://patrick-tssn.github.io/">Yuxuan Wang</a>.</p>
        </div>
      </div>
    </footer>
  </div>
</body>


<script>
  const app = {
    data() {
      return {
        selectedImage: 'assets/teaser.JPG',
      };
    },
    computed: {
      // Compute the alt text based on the selected image
      selectedImageAlt() {
        const altTexts = {
          'assets/teaser.JPG': 'assets/teaser.JPG',
          'assets/sunburst.png': 'assets/sunburst.png',
          // Add alt text for additional images
        };
        return altTexts[this.selectedImage] || 'Selected Image';
      }
    }
  };

  Vue.createApp(app).mount('#app');
</script>

</html>
